# Yaw Target Tracking Training Configuration
# Train a neural network to control drone yaw to face a moving target

environment:
  type: "yaw_tracking"
  
  # Simulation settings
  model: "generic"
  physics_timestep: 0.002  # 500 Hz physics
  control_frequency: 100.0  # 100 Hz control (increased for stability)
  max_episode_steps: 2000  # 20 seconds per episode at 100Hz
  
  # Hover settings
  hover_height: 1.0
  hover_position: [0.0, 0.0]
  
  # Target motion patterns (basic + advanced)
  target_patterns:
    # Basic patterns
    - "circular"
    - "random"
    - "sinusoidal"
    - "step"
    # Advanced patterns
    - "figure8"
    - "spiral"
    - "evasive"
    - "lissajous"
    - "multi_frequency"
  target_radius: 3.0  # meters from drone
  
  # Target speed range (used for curriculum) - start slow for stability
  target_speed_min: 0.1  # rad/s (initial) - very slow to start
  target_speed_max: 0.3  # rad/s (final) - moderate speed
  
  # Action scaling - conservative for stable tracking
  max_yaw_rate: 1.0  # rad/s - slow and stable
  action_dead_zone: 0.08  # Increased dead zone to prevent jitter
  
  # Observation noise (simulates sensor noise)
  yaw_noise: 0.01
  angular_velocity_noise: 0.01
  
  # Reward function weights (v2 - zone-based with shaping)
  rewards:
    # Main facing reward (zone-based)
    facing_reward_weight: 1.5      # Increased for zone-based rewards
    facing_reward_scale: 5.0       # exp(-scale * error^2) - kept for v1 compat
    
    # NEW: Shaping rewards
    error_reduction_weight: 0.5    # Reward for reducing error
    velocity_match_weight: 0.2     # Reward for matching target velocity
    direction_alignment_bonus: 0.1 # Bonus for turning toward target
    excess_yaw_rate_penalty: 0.05  # Only penalize excessive yaw rate
    
    # Legacy (kept for v1 compat)
    yaw_rate_penalty_weight: 0.1   # Penalize high yaw rates (v1)
    
    # Smoothness and tracking
    action_rate_penalty_weight: 0.03  # Reduced from 0.05
    sustained_tracking_bonus: 0.3  # Reduced from 0.5 (now continuous)
    sustained_tracking_threshold: 0.1  # 0.1 rad = ~6 degrees
    sustained_tracking_time: 0.5   # seconds on target for bonus
    crash_penalty: 50.0            # Strongly discourage crashes (updated from 10.0)
    alive_bonus: 0.1               # Reward survival (updated from 0.05)
    
    # Zone thresholds for facing reward
    on_target_threshold: 0.1       # 6째 - high reward zone
    close_tracking_threshold: 0.35 # 20째 - positive reward zone
    searching_threshold: 1.57      # 90째 - negative reward zone
  
  # Success criteria
  success_threshold: 0.1  # radians (~6 degrees)
  
  # Termination conditions - balanced for stability
  max_tilt_angle: 0.6  # radians (~34 degrees) - stricter to learn stable behavior
  max_altitude_error: 2.0  # meters
  
  # SITL-style PID stabilizer (tuned for stability and low crash rate)
  # See docs/issues/003-hover-pid-instability.md for tuning details
  stabilizer:
    # Altitude PID - more aggressive to maintain hover
    altitude_kp: 18.0   # increased from 15.0
    altitude_ki: 2.0    # reduced from 3.0 to prevent overshoot
    altitude_kd: 10.0   # increased from 8.0 for damping
    
    # Attitude PID (roll/pitch) - tuned for stability
    attitude_kp: 12.0   # reduced from 15.0 for smoother response
    attitude_ki: 0.3    # reduced from 0.5 to prevent windup
    attitude_kd: 8.0    # increased from 5.0 for better damping
    
    # Yaw rate control
    yaw_rate_kp: 1.5    # reduced from 2.0 for smoother yaw
    
    # Base thrust
    base_thrust: 0.62
    
    # Safety settings - very conservative for stable learning
    safety_tilt_threshold: 0.3   # radians (~17째) - stop yaw if tilted
    yaw_authority: 0.10          # very low - prioritize stability
    max_integral: 0.2            # tight anti-windup

algorithm:
  name: "PPO"
  
  # Learning parameters
  learning_rate: 3.0e-4
  n_steps: 2048           # Steps per environment before update
  batch_size: 64          # Minibatch size
  n_epochs: 10            # Epochs per update
  gamma: 0.99             # Discount factor
  gae_lambda: 0.95        # GAE lambda
  clip_range: 0.2         # PPO clip range
  ent_coef: 0.01          # Entropy coefficient (exploration)
  vf_coef: 0.5            # Value function coefficient
  max_grad_norm: 0.5      # Gradient clipping
  
  # Network architecture (larger for complex trajectories)
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256, 128]   # Policy network (larger for complex patterns)
      vf: [256, 256, 128]   # Value network
    activation_fn: "tanh"   # Smooth activation for control

training:
  # Total training timesteps (increased for complex trajectories)
  total_timesteps: 1_000_000
  
  # Parallel environments
  n_envs: 8
  
  # Random seed
  seed: 42
  
  # Checkpointing
  save_freq: 25_000       # Save every 25k steps
  checkpoint_path: "checkpoints/"
  
  # Logging
  log_interval: 10        # Log every 10 updates
  tensorboard_log: "logs/tensorboard/"
  
  # Evaluation
  eval_freq: 10_000       # Evaluate every 10k steps
  n_eval_episodes: 10     # Episodes per evaluation
  
  # Curriculum learning
  # Start with simple patterns, gradually add complexity
  curriculum:
    enabled: true
    stages:
      # Phase 1: Basic learning (0-200k)
      - timesteps: 0
        target_speed_max: 0.1
        target_patterns: ["circular"]
        description: "Stage 1: Very slow circular target"
      
      - timesteps: 50_000
        target_speed_max: 0.15
        target_patterns: ["circular", "sinusoidal"]
        description: "Stage 2: Slow, smooth patterns"
      
      - timesteps: 100_000
        target_speed_max: 0.2
        target_patterns: ["circular", "sinusoidal", "figure8"]
        description: "Stage 3: Introduce figure-8 pattern"
      
      # Phase 2: Intermediate (200k-400k)
      - timesteps: 200_000
        target_speed_max: 0.25
        target_patterns: ["circular", "sinusoidal", "figure8", "lissajous"]
        description: "Stage 4: Add Lissajous curves"
      
      - timesteps: 300_000
        target_speed_max: 0.3
        target_patterns: ["circular", "sinusoidal", "figure8", "lissajous", "spiral"]
        description: "Stage 5: Add spiral (variable radius)"
      
      # Phase 3: Advanced (400k-600k)
      - timesteps: 400_000
        target_speed_max: 0.4
        target_patterns: ["circular", "sinusoidal", "figure8", "lissajous", "spiral", "random"]
        description: "Stage 6: Add random motion"
      
      - timesteps: 500_000
        target_speed_max: 0.5
        target_patterns: ["circular", "sinusoidal", "figure8", "lissajous", "spiral", "random", "step"]
        description: "Stage 7: Add step changes"
      
      # Phase 4: Expert (600k+)
      - timesteps: 600_000
        target_speed_max: 0.6
        target_patterns: ["circular", "sinusoidal", "figure8", "lissajous", "spiral", "random", "step", "multi_frequency"]
        description: "Stage 8: Add multi-frequency patterns"
      
      - timesteps: 750_000
        target_speed_max: 0.8
        target_patterns: ["circular", "sinusoidal", "figure8", "lissajous", "spiral", "random", "step", "multi_frequency", "evasive"]
        description: "Stage 9: Full complexity - all patterns including evasive"

evaluation:
  # Evaluation settings
  n_episodes: 20
  deterministic: true
  render: false
  save_video: true
  video_fps: 30
  
  # Metrics to track
  metrics:
    - "mean_yaw_error"
    - "max_yaw_error"
    - "tracking_percentage"  # % time on target
    - "response_time"        # Time to acquire target
    - "episode_reward"
    - "episode_length"

deployment:
  # SITL deployment settings
  mavlink:
    connection: "tcp:127.0.0.1:5760"
    system_id: 1
    component_id: 1
  
  # Control settings
  control_rate: 100.0     # Hz (matches training)
  max_yaw_rate: 2.0       # rad/s
  
  # Failsafe
  timeout: 5.0            # Seconds without update before failsafe
  failsafe_action: "hold" # hold, land, or rtl
